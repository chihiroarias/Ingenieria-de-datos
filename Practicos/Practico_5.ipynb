{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3a7f16",
   "metadata": {},
   "source": [
    "## üîç Parte 1: Setup y Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab19f3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Todas las librer√≠as importadas correctamente\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "'_______' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arias\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\style\\core.py:129\u001b[39m, in \u001b[36muse\u001b[39m\u001b[34m(style)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     style = \u001b[43m_rc_params_in_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arias\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\__init__.py:903\u001b[39m, in \u001b[36m_rc_params_in_file\u001b[39m\u001b[34m(fname, transform, fail_on_error)\u001b[39m\n\u001b[32m    902\u001b[39m rc_temp = {}\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_file_or_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfd\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arias\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:141\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arias\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\__init__.py:880\u001b[39m, in \u001b[36m_open_file_or_url\u001b[39m\u001b[34m(fname)\u001b[39m\n\u001b[32m    879\u001b[39m fname = os.path.expanduser(fname)\n\u001b[32m--> \u001b[39m\u001b[32m880\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    881\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '_______'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Todas las librer√≠as importadas correctamente\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 3. Configurar visualizaciones\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m.\u001b[49m\u001b[43muse\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_______\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# estilo visual (ej: 'seaborn-v0_8', 'default', 'classic')\u001b[39;00m\n\u001b[32m     21\u001b[39m sns.set_palette(\u001b[33m\"\u001b[39m\u001b[33m_______\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# paleta de colores (ej: 'husl', 'Set1', 'viridis')\u001b[39;00m\n\u001b[32m     22\u001b[39m plt.rcParams[\u001b[33m'\u001b[39m\u001b[33mfigure.figsize\u001b[39m\u001b[33m'\u001b[39m] = (\u001b[32m12\u001b[39m, \u001b[32m8\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arias\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\style\\core.py:131\u001b[39m, in \u001b[36muse\u001b[39m\u001b[34m(style)\u001b[39m\n\u001b[32m    129\u001b[39m         style = _rc_params_in_file(style)\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    132\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstyle\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m is not a valid package style, path of style \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfile, URL of style file, or library style name (library \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstyles are listed in `style.available`)\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    135\u001b[39m filtered = {}\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m style:  \u001b[38;5;66;03m# don't trigger RcParams.__getitem__('backend')\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: '_______' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)"
     ]
    }
   ],
   "source": [
    "# === SETUP DEL ENTORNO ===\n",
    "\n",
    "# 1. Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Todas las librer√≠as importadas correctamente\")\n",
    "\n",
    "# 3. Configurar visualizaciones\n",
    "plt.style.use('_______')  # estilo visual (ej: 'seaborn-v0_8', 'default', 'classic')\n",
    "sns.set_palette(\"_______\")  # paleta de colores (ej: 'husl', 'Set1', 'viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"üé® Configuraci√≥n de visualizaciones lista!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce6856",
   "metadata": {},
   "source": [
    "## üè† Paso 2: Cargar y Crear Missing Data Sint√©tico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674f148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè† DATASET: Ames Housing\n",
      "   üìä Forma original: (2930, 82)\n",
      "   üìã Columnas: ['Order', 'PID', 'MS SubClass', 'MS Zoning', 'Lot Frontage', 'Lot Area', 'Street', 'Alley', 'Lot Shape', 'Land Contour', 'Utilities', 'Lot Config', 'Land Slope', 'Neighborhood', 'Condition 1', 'Condition 2', 'Bldg Type', 'House Style', 'Overall Qual', 'Overall Cond', 'Year Built', 'Year Remod/Add', 'Roof Style', 'Roof Matl', 'Exterior 1st', 'Exterior 2nd', 'Mas Vnr Type', 'Mas Vnr Area', 'Exter Qual', 'Exter Cond', 'Foundation', 'Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure', 'BsmtFin Type 1', 'BsmtFin SF 1', 'BsmtFin Type 2', 'BsmtFin SF 2', 'Bsmt Unf SF', 'Total Bsmt SF', 'Heating', 'Heating QC', 'Central Air', 'Electrical', '1st Flr SF', '2nd Flr SF', 'Low Qual Fin SF', 'Gr Liv Area', 'Bsmt Full Bath', 'Bsmt Half Bath', 'Full Bath', 'Half Bath', 'Bedroom AbvGr', 'Kitchen AbvGr', 'Kitchen Qual', 'TotRms AbvGrd', 'Functional', 'Fireplaces', 'Fireplace Qu', 'Garage Type', 'Garage Yr Blt', 'Garage Finish', 'Garage Cars', 'Garage Area', 'Garage Qual', 'Garage Cond', 'Paved Drive', 'Wood Deck SF', 'Open Porch SF', 'Enclosed Porch', '3Ssn Porch', 'Screen Porch', 'Pool Area', 'Pool QC', 'Fence', 'Misc Feature', 'Misc Val', 'Mo Sold', 'Yr Sold', 'Sale Type', 'Sale Condition', 'SalePrice']\n"
     ]
    }
   ],
   "source": [
    "# === CARGAR DATASET AMES HOUSING ===\n",
    "\n",
    "# 1. Cargar dataset base\n",
    "df = pd.read_csv('../datasets/AmesHousing.csv')\n",
    "\n",
    "print(\"üè† DATASET: Ames Housing\")\n",
    "print(f\"   üìä Forma original: {df.shape}\")\n",
    "print(f\"   üìã Columnas: {list(df.columns)}\")\n",
    "\n",
    "# 2. Crear missing data sint√©tico para pr√°ctica\n",
    "np.random.seed(42)  # para reproducibilidad\n",
    "\n",
    "# Simular MCAR en Year Built (8% missing aleatorio)\n",
    "# \"Los valores faltan al azar: que falte un Year Built no depende de la edad ni del propio Year Built\"\n",
    "missing_year = np.random.random(len(df)) < 0.08\n",
    "df.loc[missing_year, 'Year Built'] = np._____\n",
    "\n",
    "# Simular MAR en Garage Area (missing relacionado con Garage Type)\n",
    "# \"Los faltantes de Garage Area se concentran en ciertos tipos de garaje (variable observada)\"\n",
    "df.loc[df['Garage Type'] == 'None', 'Garage Area'] = df.loc[df['Garage Type'] == 'None', 'Garage Area'].sample(frac=0.7, random_state=42)\n",
    "\n",
    "# Simular MNAR en SalePrice (missing relacionado con precio alto)\n",
    "# \"Los faltantes dependen del propio valor: quienes tienen precios altos no reportan precio\"\n",
    "high_price = df['SalePrice'] > df['SalePrice'].quantile(0.85)\n",
    "df.loc[high_price, 'SalePrice'] = df.loc[high_price, 'SalePrice'].sample(frac=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nüîç Missing data sint√©tico creado:\")\n",
    "print(df._____().sum())  # m√©todo para contar valores faltantes por columna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7dc778",
   "metadata": {},
   "source": [
    "## üìä Paso 3: An√°lisis Inicial del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55feb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXPLORACI√ìN B√ÅSICA ===\n",
    "\n",
    "# 1. Informaci√≥n general del dataset\n",
    "print(\"=== INFORMACI√ìN GENERAL ===\")\n",
    "print(df._____())  # m√©todo que muestra tipos de datos, memoria y valores no nulos\n",
    "\n",
    "# 2. Estad√≠sticas descriptivas\n",
    "print(\"\\n=== ESTAD√çSTICAS DESCRIPTIVAS ===\")\n",
    "print(df._____())  # m√©todo que calcula estad√≠sticas descriptivas\n",
    "\n",
    "# 3. Tipos de datos\n",
    "print(\"\\n=== TIPOS DE DATOS ===\")\n",
    "print(df._____)  # atributo que muestra tipos de datos por columna\n",
    "\n",
    "# 4. Verificar missing data\n",
    "print(\"\\n=== MISSING DATA POR COLUMNA ===\")\n",
    "missing_count = df._____().sum()  # contar valores faltantes\n",
    "missing_pct = (missing_count / len(df)) * 100  # calcular porcentaje\n",
    "\n",
    "missing_stats = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': missing_count,\n",
    "    'Missing_Percentage': missing_pct\n",
    "})\n",
    "print(missing_stats[missing_stats['Missing_Count'] > 0])\n",
    "\n",
    "# 5. An√°lisis de memoria\n",
    "print(\"\\n=== AN√ÅLISIS DE MEMORIA ===\")\n",
    "total_bytes = df._____(deep=True).sum()  # m√©todo para memoria en bytes\n",
    "print(f\"Memoria total del DataFrame: {total_bytes / (1024**2):.2f} MB\")\n",
    "print(f\"Memoria por columna:\")\n",
    "for col in df.columns:\n",
    "    memory_usage = df[col]._____()  # m√©todo para memoria de una columna\n",
    "    print(f\"  {col}: {memory_usage / 1024:.2f} KB\")\n",
    "\n",
    "# 6. An√°lisis de duplicados\n",
    "print(\"\\n=== AN√ÅLISIS DE DUPLICADOS ===\")\n",
    "duplicates = df._____()  # m√©todo para detectar filas duplicadas\n",
    "print(f\"N√∫mero de filas duplicadas: {duplicates.sum()}\")\n",
    "if duplicates.sum() > 0:\n",
    "    print(\"Primeras 5 filas duplicadas:\")\n",
    "    print(df[df._____()].head())  # m√©todo para filtrar duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56276851",
   "metadata": {},
   "source": [
    "## üîç Paso 4: Detecci√≥n de Patrones de Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3140f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AN√ÅLISIS DE PATRONES DE MISSING DATA ===\n",
    "\n",
    "# 1. Filtrar solo columnas con missing data para visualizaci√≥n\n",
    "missing_columns = df.columns[df._____().any()].tolist()  # m√©todo para detectar missing\n",
    "print(f\"Columnas con missing data: {len(missing_columns)}\")\n",
    "print(f\"Columnas: {missing_columns}\")\n",
    "\n",
    "# 2. Visualizaci√≥n mejorada sin missingno\n",
    "plt.subplot(1, 1, 1)\n",
    "if len(missing_columns) > 0:\n",
    "    # Crear estad√≠sticas de missing solo para columnas con missing data\n",
    "    missing_count = df[missing_columns]._____().sum()  # m√©todo para contar missing\n",
    "    missing_pct = (missing_count / len(df)) * 100  # calcular porcentaje\n",
    "\n",
    "    missing_stats_filtered = pd.DataFrame({\n",
    "        'Column': missing_columns,\n",
    "        'Missing_Count': missing_count,\n",
    "        'Missing_Percentage': missing_pct\n",
    "    }).sort_values('Missing_Percentage', ascending=False).head(10)\n",
    "\n",
    "    # Crear gr√°fico de barras m√°s limpio\n",
    "    bars = plt._____(range(len(missing_stats_filtered)), missing_stats_filtered['Missing_Percentage'], \n",
    "                   color='steelblue', alpha=0.7, edgecolor='black', linewidth=0.5)  # funci√≥n para barras\n",
    "    plt.title('Top 10: Porcentaje de Missing por Columna', fontsize=14, fontweight='bold')\n",
    "    plt._____(range(len(missing_stats_filtered)), missing_stats_filtered['Column'], \n",
    "               rotation=45, ha='right')  # funci√≥n para etiquetas del eje X\n",
    "\n",
    "    plt.ylabel('Porcentaje de Missing (%)')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Agregar valores en las barras\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No hay missing data', ha='center', va='center', fontsize=16)\n",
    "    plt.title('Porcentaje de Missing por Columna', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Distribuci√≥n de missing por fila\n",
    "plt.show()\n",
    "plt.subplot(1, 1, 1)\n",
    "missing_per_row = df._____().sum(axis=1)  # contar missing por fila\n",
    "plt._____(missing_per_row, bins=range(0, missing_per_row.max()+2), alpha=0.7, \n",
    "         edgecolor='black', color='lightcoral')  # funci√≥n para histograma\n",
    "plt.title('Distribuci√≥n de Missing por Fila', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('N√∫mero de valores faltantes por fila')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "!mkdir -p results/visualizaciones\n",
    "plt.savefig('results/visualizaciones/missing_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4135391",
   "metadata": {},
   "source": [
    "## üß† Paso 5: Clasificar Tipos de Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca32295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CLASIFICACI√ìN MCAR/MAR/MNAR ===\n",
    "\n",
    "print(\"=== AN√ÅLISIS DE TIPOS DE MISSING ===\")\n",
    "\n",
    "# 1. Year Built: ¬øMCAR o MAR?\n",
    "print(\"\\n1. YEAR BUILT - An√°lisis de patrones:\")\n",
    "year_missing = df['Year Built']._____()  # m√©todo para detectar missing\n",
    "print(\"Missing Year Built por Neighborhood:\")\n",
    "print(df.groupby('Neighborhood')['Year Built'].apply(lambda x: x._____().sum()))  # contar missing por grupo\n",
    "\n",
    "print(\"Missing Year Built por House Style:\")\n",
    "print(df.groupby('House Style')['Year Built'].apply(lambda x: x._____().sum()))\n",
    "\n",
    "# 2. Garage Area: ¬øMAR?\n",
    "print(\"\\n2. GARAGE AREA - An√°lisis de patrones:\")\n",
    "print(\"Missing Garage Area por Garage Type:\")\n",
    "print(df.groupby('Garage Type')['Garage Area'].apply(lambda x: x._____().sum()))\n",
    "\n",
    "# 3. SalePrice: ¬øMNAR?\n",
    "print(\"\\n3. SALEPRICE - An√°lisis de patrones:\")\n",
    "price_missing = df['SalePrice']._____()\n",
    "print(\"Valores de SalePrice en registros con missing:\")\n",
    "print(df[price_missing]['SalePrice']._____())  # estad√≠sticas descriptivas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4d6c12",
   "metadata": {},
   "source": [
    "## üö® Paso 6: Detecci√≥n de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33393285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DETECCI√ìN DE OUTLIERS CON IQR ===\n",
    "# \"Detectar extremos usando mediana y cuartiles\"\n",
    "# \"Cu√°ndo usar: distribuciones asim√©tricas / colas pesadas / presencia de outliers\"\n",
    "if \"Year Built\" in df.columns:\n",
    "    df[\"Year Built\"] = pd.to_numeric(df[\"Year Built\"], errors=\"coerce\")\n",
    "\n",
    "# === DETECCI√ìN DE OUTLIERS: IQR y Z-SCORE (robustas) ===\n",
    "def detect_outliers_iqr(df, column, factor=1.5):\n",
    "    \"\"\"Outliers por IQR. Devuelve (df_outliers, lower, upper).\"\"\"\n",
    "    x = pd.to_numeric(df[column], errors=\"coerce\")\n",
    "    x_no_na = x.dropna().astype(float).values\n",
    "    if x_no_na.size == 0:\n",
    "        # sin datos v√°lidos\n",
    "        return df.iloc[[]], np.nan, np.nan\n",
    "    q1 = np.percentile(x_no_na, 25)\n",
    "    q3 = np.percentile(x_no_na, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - factor * iqr\n",
    "    upper = q3 + factor * iqr\n",
    "    mask = (pd.to_numeric(df[column], errors=\"coerce\") < lower) | (pd.to_numeric(df[column], errors=\"coerce\") > upper)\n",
    "    return df[mask], lower, upper\n",
    "\n",
    "# Analizar outliers en columnas num√©ricas\n",
    "numeric_columns = df._____(include=[np.number]).columns  # m√©todo para seleccionar columnas num√©ricas\n",
    "outlier_analysis = {}\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if not df[col]._____().all():  # m√©todo para verificar si hay missing data\n",
    "        outliers, lower, upper = detect_outliers_iqr(df, col)\n",
    "        outlier_analysis[col] = {\n",
    "            'count': len(outliers),\n",
    "            'percentage': (len(outliers) / len(df)) * 100,\n",
    "            'lower_bound': lower,\n",
    "            'upper_bound': upper\n",
    "        }\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_analysis).T\n",
    "print(\"=== AN√ÅLISIS DE OUTLIERS (IQR) ===\")\n",
    "print(\"√ötil cuando la distribuci√≥n est√° chueca o con colas largas\")\n",
    "print(outlier_df)\n",
    "\n",
    "# An√°lisis adicional de outliers\n",
    "print(\"\\n=== RESUMEN DE OUTLIERS ===\")\n",
    "total_outliers = outlier_df['count']._____()  # m√©todo para sumar outliers\n",
    "print(f\"Total de outliers detectados: {total_outliers}\")\n",
    "print(f\"Porcentaje promedio de outliers: {outlier_df['percentage']._____():.2f}%\")  # m√©todo para calcular media\n",
    "print(f\"Columna con m√°s outliers: {outlier_df['count']._____()}\")  # m√©todo para encontrar m√°ximo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91dfb23",
   "metadata": {},
   "source": [
    "## üîç Paso 7: Detecci√≥n de Outliers con Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DETECCI√ìN DE OUTLIERS CON Z-SCORE ===\n",
    "# \"Cu√°ndo usar: distribuci√≥n aprox. campana y sin colas raras\"\n",
    "# \"Regla: 3 pasos (desvios) desde el promedio = raro\"\n",
    "\n",
    "def detect_outliers_zscore(df, column, threshold=3):\n",
    "    \"\"\"Detectar outliers usando Z-Score - Regla: 3 desvios desde el promedio = raro\"\"\"\n",
    "    from scipy import stats\n",
    "    z_scores = np.abs(stats.zscore(df[column].dropna()))\n",
    "    outlier_indices = df[column].dropna().index[z_scores > threshold]\n",
    "    return df.loc[outlier_indices]\n",
    "\n",
    "# Comparar m√©todos de detecci√≥n\n",
    "print(\"\\n=== COMPARACI√ìN DE M√âTODOS DE DETECCI√ìN ===\")\n",
    "for col in ['SalePrice', 'Lot Area', 'Year Built', 'Garage Area']:\n",
    "    if col in df.columns and not df[col].isnull().all():\n",
    "        iqr_outliers = detect_outliers_iqr(df, col)\n",
    "        zscore_outliers = detect_outliers_zscore(df, col)\n",
    "\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  IQR outliers: {len(iqr_outliers[0])} ({len(iqr_outliers[0])/len(df)*100:.1f}%)\")\n",
    "        print(f\"  Z-Score outliers: {len(zscore_outliers)} ({len(zscore_outliers)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903562c",
   "metadata": {},
   "source": [
    "## üìä Paso 8: Visualizaci√≥n de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b4620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VISUALIZAR OUTLIERS ===\n",
    "\n",
    "# Crear directorio para resultados\n",
    "import os\n",
    "os._____('results/visualizaciones', exist_ok=True)  # m√©todo para crear directorio\n",
    "\n",
    "# Visualizar outliers con boxplots\n",
    "fig, axes = plt._____(2, 2, figsize=(15, 12))  # funci√≥n para crear subplots\n",
    "axes = axes._____()  # m√©todo para aplanar array\n",
    "\n",
    "for i, col in enumerate(['SalePrice', 'Lot Area', 'Year Built', 'Garage Area']):\n",
    "    if col in df.columns and not df[col]._____().all():  # m√©todo para verificar missing\n",
    "        # Boxplot\n",
    "        sns._____(data=df, y=col, ax=axes[i])  # funci√≥n para boxplot\n",
    "        axes[i].set_title(f'Outliers en {col}', fontweight='bold')\n",
    "        axes[i].set_ylabel(col)\n",
    "\n",
    "        # Marcar outliers\n",
    "        outliers, lower, upper = detect_outliers_iqr(df, col)\n",
    "        if len(outliers) > 0:\n",
    "            axes[i]._____([0] * len(outliers), outliers[col], \n",
    "                           color='red', alpha=0.6, s=50, label=f'Outliers ({len(outliers)})')  # funci√≥n para scatter\n",
    "            axes[i]._____()  # m√©todo para mostrar leyenda\n",
    "\n",
    "plt._____()  # funci√≥n para ajustar layout\n",
    "plt._____('results/visualizaciones/outliers_analysis.png', dpi=300, bbox_inches='tight')  # funci√≥n para guardar\n",
    "plt._____()  # funci√≥n para mostrar gr√°fico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e73bb",
   "metadata": {},
   "source": [
    "## üîß Paso 9: Estrategias de Imputaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf7190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPLEMENTAR ESTRATEGIAS DE IMPUTACI√ìN ===\n",
    "# \"Rellenar no es gratis; hacelo columna a columna y document√°\"\n",
    "# \"Num: mediana (si cola pesada) / media (si ~normal)\"\n",
    "# \"Cat: moda o 'Unknown' (+ flag si sospecha MNAR)\"\n",
    "\n",
    "def impute_missing_data(df, strategy='median'):\n",
    "    \"\"\"Implementar diferentes estrategias de imputaci√≥n - Reglas simples de la clase\"\"\"\n",
    "    df_imputed = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any():\n",
    "            if df[col].dtype in ['int64', 'float64']:\n",
    "                if strategy == 'mean':\n",
    "                    df_imputed[col].fillna(df[col]._____(), inplace=True)  # imputar con media\n",
    "                elif strategy == 'median':\n",
    "                    df_imputed[col].fillna(df[col]._____(), inplace=True)  # imputar con mediana\n",
    "                elif strategy == 'mode':\n",
    "                    df_imputed[col].fillna(df[col]._____()[0], inplace=True)  # imputar con moda\n",
    "            else:\n",
    "                # Para variables categ√≥ricas\n",
    "                df_imputed[col].fillna(df[col]._____()[0], inplace=True)  # imputar con moda\n",
    "\n",
    "    return df_imputed\n",
    "\n",
    "# Probar diferentes estrategias\n",
    "strategies = ['mean', 'median', 'mode']\n",
    "imputed_datasets = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    imputed_datasets[strategy] = impute_missing_data(df, strategy)\n",
    "    print(f\"Estrategia {strategy}: {imputed_datasets[strategy].isnull().sum().sum()} missing values restantes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2b679",
   "metadata": {},
   "source": [
    "## üß† Paso 10: Imputaci√≥n Inteligente por Tipo de Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPUTACI√ìN INTELIGENTE ===\n",
    "\n",
    "def smart_imputation(df):\n",
    "    \"\"\"Imputaci√≥n inteligente basada en el tipo de missing data\"\"\"\n",
    "    df_imputed = df.copy()\n",
    "\n",
    "    # Year Built: MAR - imputar por Neighborhood y House Style\n",
    "    year_by_group = df.groupby(['Neighborhood', 'House Style'])['Year Built']._____()  # mediana por grupo\n",
    "    for (neighborhood, style), median_year in year_by_group.items():\n",
    "        mask = (df['Neighborhood'] == neighborhood) & (df['House Style'] == style) & df['Year Built']._____()  # condici√≥n para missing\n",
    "        df_imputed.loc[mask, 'Year Built'] = median_year\n",
    "\n",
    "    # Garage Area: Crear categor√≠a \"Unknown\" para MNAR\n",
    "    df_imputed['Garage Area'].fillna('_____', inplace=True)  # valor para missing\n",
    "\n",
    "    # SalePrice: Imputar con mediana por Neighborhood\n",
    "    price_by_neighborhood = df.groupby('Neighborhood')['SalePrice']._____()  # mediana por Neighborhood\n",
    "    for neighborhood, median_price in price_by_neighborhood.items():\n",
    "        mask = (df['Neighborhood'] == neighborhood) & df['SalePrice']._____()  # condici√≥n para missing\n",
    "        df_imputed.loc[mask, 'SalePrice'] = median_price\n",
    "\n",
    "    # Garage Type: Imputar con moda (MCAR)\n",
    "    df_imputed['Garage Type'].fillna(df['Garage Type']._____()[0], inplace=True)  # moda\n",
    "\n",
    "    return df_imputed\n",
    "\n",
    "# Aplicar imputaci√≥n inteligente\n",
    "df_smart_imputed = smart_imputation(df)\n",
    "print(\"=== IMPUTACI√ìN INTELIGENTE ===\")\n",
    "print(f\"Missing values restantes: {df_smart_imputed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f622b171",
   "metadata": {},
   "source": [
    "## üö´ Paso 11: Anti-Leakage B√°sico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ANTI-LEAKAGE B√ÅSICO ===\n",
    "# \"No espi√©s el examen: fit en TRAIN, transform en VALID/TEST\"\n",
    "# \"Split: X_train / X_valid / X_test\"\n",
    "# \"imputer.fit(X_train) ‚Üí transform al resto\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Split de datos (ANTES de imputar)\n",
    "X = df.drop('SalePrice', axis=1)  # features\n",
    "y = df['SalePrice']  # target\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"=== SPLIT DE DATOS ===\")\n",
    "print(f\"Train: {X_train.shape[0]} registros\")\n",
    "print(f\"Valid: {X_valid.shape[0]} registros\") \n",
    "print(f\"Test: {X_test.shape[0]} registros\")\n",
    "\n",
    "# 2. Imputar SOLO en train, luego transformar\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Separar columnas num√©ricas y categ√≥ricas\n",
    "numeric_columns = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Columnas num√©ricas: {len(numeric_columns)}\")\n",
    "print(f\"Columnas categ√≥ricas: {len(categorical_columns)}\")\n",
    "\n",
    "# Crear imputers para cada tipo de dato\n",
    "numeric_imputer = SimpleImputer(strategy='_____')  # estrategia para num√©ricas\n",
    "categorical_imputer = SimpleImputer(strategy='_____')  # estrategia para categ√≥ricas\n",
    "\n",
    "# Ajustar imputers SOLO con train\n",
    "numeric_imputer._____(X_train[numeric_columns])  # ajustar num√©ricas\n",
    "categorical_imputer._____(X_train[categorical_columns])  # ajustar categ√≥ricas\n",
    "\n",
    "# Transformar todos los conjuntos\n",
    "X_train_numeric = numeric_imputer._____(X_train[numeric_columns])  # transformar num√©ricas\n",
    "X_train_categorical = categorical_imputer._____(X_train[categorical_columns])  # transformar categ√≥ricas\n",
    "\n",
    "X_valid_numeric = numeric_imputer._____(X_valid[numeric_columns])\n",
    "X_valid_categorical = categorical_imputer._____(X_valid[categorical_columns])\n",
    "\n",
    "X_test_numeric = numeric_imputer._____(X_test[numeric_columns])\n",
    "X_test_categorical = categorical_imputer._____(X_test[categorical_columns])\n",
    "\n",
    "print(\"\\n‚úÖ Anti-leakage aplicado: fit solo en train, transform en todo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9d9e4",
   "metadata": {},
   "source": [
    "## üìä Paso 12: An√°lisis de Impacto de la Imputaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Crear df_imputed con imputaci√≥n simple, robusta a dtypes\n",
    "df_imputed = df.copy()\n",
    "\n",
    "for col in df.columns:\n",
    "    s = df[col]\n",
    "    # Si es num√©rica o puede convertirse a num√©rica, imputar mediana\n",
    "    if is_numeric_dtype(s) or (s.dtype == \"object\"):\n",
    "        s_num = pd.to_numeric(s, errors=\"coerce\")\n",
    "        if s_num.notna().any():\n",
    "            df_imputed[col] = s_num.fillna(s_num._____()) # imputar num√©ricas con mediana\n",
    "            continue\n",
    "    # Caso categ√≥rico: imputar con moda (si existe), sino \"Unknown\"\n",
    "    moda = s.dropna()._____() # imputar categ√≥ricas con moda\n",
    "    fill_val = moda.iloc[0] if not moda.empty else \"Unknown\"\n",
    "    df_imputed[col] = s.fillna(fill_val)\n",
    "\n",
    "# 2) Comparar distribuciones (hist para num√©ricas, barras para categ√≥ricas)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "cols_to_plot = ['SalePrice', 'Lot Area', 'Year Built', 'Garage Area', 'Neighborhood', 'House Style']\n",
    "for i, col in enumerate(cols_to_plot):\n",
    "    if col not in df.columns:\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'{col} no existe', fontweight='bold')\n",
    "        continue\n",
    "\n",
    "    s_orig = df[col]\n",
    "    s_imp = df_imputed[col]\n",
    "\n",
    "    # Intentar tratar como num√©rico (coerce) para decidir el tipo de gr√°fico\n",
    "    s_orig_num = pd.to_numeric(s_orig, errors='coerce')\n",
    "    s_imp_num = pd.to_numeric(s_imp, errors='coerce')\n",
    "\n",
    "    if s_orig_num.notna().any() and s_imp_num.notna().any():\n",
    "        # NUM√âRICAS ‚Üí hist\n",
    "        # Mismo rango/bins para una comparaci√≥n justa\n",
    "        data_combined = pd.concat([s_orig_num.dropna(), s_imp_num.dropna()])\n",
    "        bins = np.histogram_bin_edges(data_combined, bins=20)\n",
    "\n",
    "        axes[i].hist(s_orig_num.dropna(), bins=bins, alpha=0.9, label='Original',\n",
    "                     color='steelblue', edgecolor='black')\n",
    "        axes[i].hist(s_imp_num.dropna(), bins=bins, alpha=0.3, label='Imputado',\n",
    "                     color='orange', edgecolor='black')\n",
    "\n",
    "        # Si est√° muy sesgada, te puede servir escala log\n",
    "        if col in ['Lot Area', 'SalePrice'] and s_orig_num.dropna().skew() > 1:\n",
    "            axes[i].set_yscale('log')\n",
    "\n",
    "    else:\n",
    "        # CATEG√ìRICAS ‚Üí barras (top-K categor√≠as para legibilidad)\n",
    "        K = 12\n",
    "        vc_orig = s_orig.astype('object').fillna('Missing').value_counts().head(K)\n",
    "        vc_imp  = s_imp.astype('object').fillna('Missing').value_counts().head(K)\n",
    "        cats = list(dict.fromkeys(list(vc_orig.index) + list(vc_imp.index)))[:K]  # uni√≥n ordenada\n",
    "\n",
    "        vc_orig = vc_orig.reindex(cats, fill_value=0)\n",
    "        vc_imp  = vc_imp.reindex(cats, fill_value=0)\n",
    "\n",
    "        x = np.arange(len(cats))\n",
    "        w = 0.4\n",
    "        axes[i].bar(x - w/2, vc_orig.values, width=w, label='Original')\n",
    "        axes[i].bar(x + w/2, vc_imp.values,  width=w, label='Imputado')\n",
    "        axes[i].set_xticks(x)\n",
    "        axes[i].set_xticklabels(cats, rotation=30, ha='right')\n",
    "\n",
    "    axes[i].set_title(f'Distribuci√≥n de {col}', fontweight='bold')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/visualizaciones/distribution_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3) Correlaciones (solo num√©ricas y con coerci√≥n segura)\n",
    "important_cols = ['SalePrice', 'Lot Area', 'Year Built', 'Garage Area', 'Overall Qual', 'Gr Liv Area', 'Total Bsmt SF']\n",
    "available_cols = [c for c in important_cols if c in df.columns]\n",
    "print(f\"Columnas seleccionadas para correlaciones: {available_cols}\")\n",
    "\n",
    "df_num_original = df[available_cols].apply(pd.to_numeric, errors='coerce')\n",
    "df_num_imputed  = df_imputed[available_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "corr_original = df_num_original._____(numeric_only=True)  # m√©todo para matriz de correlaci√≥n\n",
    "sns.heatmap(corr_original, annot=True, cmap='coolwarm', center=0, ax=axes[0],\n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "axes[0].set_title('Correlaciones - Original', fontweight='bold', fontsize=14)\n",
    "\n",
    "corr_imputed = df_num_imputed._____(numeric_only=True)  # m√©todo para matriz de correlaci√≥n\n",
    "sns.heatmap(corr_imputed, annot=True, cmap='coolwarm', center=0, ax=axes[1],\n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "axes[1].set_title('Correlaciones - Imputado', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/visualizaciones/correlation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 4) Diferencias en correlaciones\n",
    "print(\"\\n=== DIFERENCIAS EN CORRELACIONES ===\")\n",
    "corr_diff = corr_imputed - corr_original\n",
    "print(\"Cambios en correlaciones (Imputado - Original):\")\n",
    "print(corr_diff.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0aaad",
   "metadata": {},
   "source": [
    "## üîß Paso 13: Pipeline de Limpieza Reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a02fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREAR PIPELINE CON SKLEARN ===\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def create_cleaning_pipeline():\n",
    "    \"\"\"Crear pipeline de limpieza reproducible\"\"\"\n",
    "\n",
    "    # Definir columnas num√©ricas y categ√≥ricas\n",
    "    numeric_features = ['SalePrice', 'Lot Area', 'Year Built', 'Garage Area']\n",
    "    categorical_features = ['Neighborhood', 'House Style', 'Garage Type']\n",
    "\n",
    "    # Transformadores\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='_____')),  # estrategia de imputaci√≥n\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='_____')),  # estrategia de imputaci√≥n\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Combinar transformadores\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "# Crear y probar pipeline\n",
    "preprocessor = create_cleaning_pipeline()\n",
    "\n",
    "# Aplicar pipeline\n",
    "X_cleaned = preprocessor._____(df)  # m√©todo para aplicar transformaciones\n",
    "print(f\"Shape despu√©s del pipeline: {X_cleaned.shape}\")\n",
    "print(f\"Tipo de datos: {type(X_cleaned)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
